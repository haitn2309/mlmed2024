# -*- coding: utf-8 -*-
"""ECG Heartbeat Categorization Practice

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1spdjn6jIa41keSYHM4DxwwDlQ6K-OL3Z
"""

from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import os # accessing directory structure
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

nRowsRead = 1000
df1 = pd.read_csv('mitbih_test.csv', delimiter=',', nrows = nRowsRead)
df1.head()

nRowsRead = 1000
df2 = pd.read_csv('mitbih_train.csv', delimiter=',', nrows = nRowsRead)
df2.head()

# Distribution graphs (histogram/bar graph) of column data
def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):
    nunique = df.nunique()
    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values
    nRow, nCol = df.shape
    columnNames = list(df)
    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow
    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
    for i in range(min(nCol, nGraphShown)):
        plt.subplot(nGraphRow, nGraphPerRow, i + 1)
        columnDf = df.iloc[:, i]
        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):
            valueCounts = columnDf.value_counts()
            valueCounts.plot.bar()
        else:
            columnDf.hist()
        plt.ylabel('counts')
        plt.xticks(rotation = 90)
        plt.title(f'{columnNames[i]} (column {i})')
    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
    plt.show()

plotPerColumnDistribution(df, 10, 2)

import matplotlib.pyplot as plt
import pandas as pd

def plotCorrelationMatrix(df, graphWidth, title="Correlation Matrix"):
    df = df.dropna(axis='columns')  # Drop columns with NaN
    df = df[[col for col in df if df[col].nunique() > 1]]  # Keep columns where there are more than 1 unique values
    if df.shape[1] < 2:
        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
        return
    corr = df.corr()
    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')
    corrMat = plt.matshow(corr, fignum=1)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)
    plt.gca().xaxis.tick_bottom()
    plt.colorbar(corrMat)
    plt.title(title, fontsize=15)
    plt.show()

# Assuming your DataFrame is loaded into `df` and you want a graphWidth of 10
plotCorrelationMatrix(df, 10, "CorrelationMatrix")

# Scatter and density plots
def plotScatterMatrix(df, plotSize, textSize):
    df = df.select_dtypes(include =[np.number]) # keep only numerical columns
    # Remove rows and columns that would lead to df being singular
    df = df.dropna('columns')
    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
    columnNames = list(df)
    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots
        columnNames = columnNames[:10]
    df = df[columnNames]
    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
    corrs = df.corr().values
    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):
        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
    plt.suptitle('Scatter and Density Plot')
    plt.show()

plotScatterMatrix(df1, 20, 10)

df = pd.concat([df1, df2], ignore_index=True)

# Display basic information about the dataset
df.info()

# Display descriptive statistics
df.describe()

# Check for missing values
df.isnull().sum()

test_df = df.head()

# Display the first few rows of the training set and test set
train_df = df.head()
test_df = test_df.head()

#Split the dataset into features and labels (the last column will be used as labels)
X_train = train_df.iloc[:, :-1].values
y_train = train_df.iloc[:, -1].values
X_test = test_df.iloc[:, :-1].values
y_test = test_df.iloc[:, -1].values

print(df.head())
print(test_df.head())

from sklearn.model_selection import train_test_split

X = df.iloc[:, :-1]  # all columns except the last
y = df.iloc[:, -1]  # the last column

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

# If y_train and y_test need to be re-encoded to start from 0, consider using LabelEncoder from sklearn

# Calculate the number of unique classes for the output layer
n_classes = np.unique(y_train).shape[0]

# Define the model
model = models.Sequential([
  layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
  layers.Dense(64, activation='relu'),
  layers.Dense(n_classes, activation='softmax')  # Use n_classes for the output layer
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Ensure X_train and y_train are numpy arrays for compatibility with TensorFlow
# X_train = np.array(X_train)
# y_train = np.array(y_train)

# Train the model
history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)

# Ensure X_test and y_test are numpy arrays for evaluation
# X_test = np.array(X_test)
# y_test = np.array(y_test)

# Evaluate the model
model.evaluate(X_test, y_test)

# Print unique values in y_train and y_test
print("Unique values in y_train:", np.unique(y_train))
print("Unique values in y_test:", np.unique(y_test))

# Clip labels to the valid range
y_train = np.clip(y_train, 0, n_classes - 1)
y_test = np.clip(y_test, 0, n_classes - 1)

# Convert labels to integers if necessary
y_train = y_train.astype(np.int32)
y_test = y_test.astype(np.int32)

print(y_train.isna().sum())
print(y_test.isna().sum())

print(np.isfinite(y_train).all())
print(np.isfinite(y_test).all())

# Replace missing values with a valid integer
y_train = y_train.fillna(0).astype(np.int32)
y_test = y_test.fillna(0).astype(np.int32)

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history['accuracy'], label='Train')  # Changed from history.history['accuracy']
plt.plot(history['val_accuracy'], label='Validation')  # Changed from history.history['val_accuracy']
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history['loss'], label='Train')  # Changed from history.history['loss']
plt.plot(history['val_loss'], label='Validation')  # Changed from history.history['val_loss']
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

# Calculate predictions
predictions = model.predict(X_test)

# For a binary classification problem, you might interpret the predictions like this:
predicted_classes_binary = (predictions > 0.5).astype(int)

# For a multiclass classification problem, you find the class with the highest probability:
predicted_classes_multiclass = np.argmax(predictions, axis=1)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)

print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
# Calculate metrics
accuracy = accuracy_score(y_test, predicted_classes_binary)
precision = precision_score(y_test, predicted_classes_binary)
recall = recall_score(y_test, predicted_classes_binary)
f1 = f1_score(y_test, predicted_classes_binary)

# Print metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Confusion Matrix
cm = confusion_matrix(y_test, predicted_classes_binary)
print("Confusion Matrix:")
print(cm)

# Calculate metrics
accuracy = accuracy_score(y_test, predicted_classes_multiclass)
precision = precision_score(y_test, predicted_classes_multiclass, average='macro')
recall = recall_score(y_test, predicted_classes_multiclass, average='macro')
f1 = f1_score(y_test, predicted_classes_multiclass, average='macro')

# Print metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

# Confusion Matrix
cm = confusion_matrix(y_test, predicted_classes_multiclass)
print("Confusion Matrix:")
print(cm)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns  # For a nicer confusion matrix visualization

# Commented out IPython magic to ensure Python compatibility.
# %pip install matplotlib scikit-learn

# Compute the confusion matrix
cm = confusion_matrix(y_test, predicted_classes_multiclass)

# For multiclass, ensure that you have predicted_classes that match the shape and type of y_test

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


# Custom labels for your classes
labels = ['Normal', 'Supraventricular ectopic', 'Ventricular ectopic', 'Fusion', 'Unknown']

# Create the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt="g", cmap="Blues", xticklabels=labels, yticklabels=labels)

# Rotate the tick labels for readability
plt.xticks(rotation=45)
plt.yticks(rotation=45)

# Label the axes and add a title
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')

# Display the plot
plt.show()

