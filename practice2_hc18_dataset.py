# -*- coding: utf-8 -*-
"""Practice2 HC18 Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/103A1ZCjvaaQNgPMcDQp982LCl0NfD2tX
"""

import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

# Set dataset path
dataset_path = '/content/drive/My Drive/HC18Dataset/'

# Load CSV
annotations_df = pd.read_csv(f'{dataset_path}training_set_pixel_size_and_HC.csv')
annotations_df

def load_and_resize_image(filename):
    image_path = os.path.join(dataset_path, 'training_set', filename)
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is not None:
        return cv2.resize(img, (800, 540))
    else:
        print(f"Failed to load image: {image_path}")
        return None

# Apply function to all filenames in the dataframe
images = [load_and_resize_image(filename) for filename in annotations_df['filename']]
images = [img for img in images if img is not None]

if not images:
    print("No images were loaded. Check file paths and names.")
else:
    # Convert list of images to a numpy array and preprocess
    images = np.array(images, dtype='float32').reshape(-1, 800, 540, 1) / 255.0
    hcs = annotations_df['head circumference (mm)'].values.astype('float32')

    # Splitting dataset
    train_images, val_images, train_hcs, val_hcs = train_test_split(images, hcs, test_size=0.2, random_state=42)

    print(f"Successfully loaded and split {len(images)} images.")
    print(f"Train set images: {len(train_images)}, Validation set images: {len(val_images)}")

data = pd.read_csv('test_set_pixel_size.csv')
data

from matplotlib import pyplot as plt
data['pixel size(mm)'].plot(kind='hist', bins=20, title='pixel size(mm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

data.head()

# @title pixel size(mm)

from matplotlib import pyplot as plt
data['pixel size(mm)'].plot(kind='line', figsize=(8, 4), title='pixel size(mm)')
plt.gca().spines[['top', 'right']].set_visible(False)

data2 = pd.read_csv('training_set_pixel_size_and_HC.csv')
data2

data2.head()

# @title head circumference (mm)

from matplotlib import pyplot as plt
data2['head circumference (mm)'].plot(kind='hist', bins=20, title='head circumference (mm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title pixel size(mm) vs head circumference (mm)

from matplotlib import pyplot as plt
data2.plot(kind='scatter', x='pixel size(mm)', y='head circumference (mm)', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

df = pd.concat([data, data2], axis=0)

df.shape

df.head()

df.info()

df.describe()

from matplotlib import pyplot as plt
_df_14['head circumference (mm)'].plot(kind='line', figsize=(8, 4), title='head circumference (mm)')
plt.gca().spines[['top', 'right']].set_visible(False)

# Histogram for head circumference
df['head circumference (mm)'].hist(bins=50)
plt.xlabel('Head Circumference (mm)')
plt.ylabel('Frequency')
plt.title('Distribution of Head Circumference Measurements')
plt.show()

# For the correlation matrix, ensure only numeric data is considered
correlation_matrix = df.corr(numeric_only=True)
print(correlation_matrix)

# When filling missing values, specify numeric_only=True to silence the warning
df.fillna(df.mean(numeric_only=True), inplace=True)

df.columns

# Correctly reference the column names with the exact spelling and spacing
features = df.drop('head circumference (mm)', axis=1)
target = df['head circumference (mm)']

# Proceed with the train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

print(X_train.dtypes)  # This will show the data type of each column in X_train

# Assuming 'filename' is the column that contains the filenames such as '766_HC.png'
features = df.drop('filename', axis=1)

# Convert all columns to numeric, coercing errors to NaNs (which you'll handle in the next step)
features = features.apply(pd.to_numeric, errors='coerce')

# Fill NaNs with the mean value of each column or another imputation strategy
features.fillna(features.mean(), inplace=True)

X = features.values  # Convert DataFrame to numpy array
y = df['head circumference (mm)'].values  # Assuming this is your target column

# Then split your data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32)

from tensorflow.keras import layers, models

def build_model(input_shape):
    model = models.Sequential()
    model.add(layers.Dense(64, activation='relu', input_shape=(input_shape,)))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(1)) # Output layer for regression

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

model = build_model(X_train.shape[1])
model.summary()

# This assumes 'filename' is the column with the image filenames.
# Make sure to replace 'filename' with the actual name of the column in your DataFrame.
features = df.drop(['head circumference (mm)', 'filename'], axis=1)

features = features.apply(pd.to_numeric, errors='coerce')  # Convert all columns to numeric, coercing errors to NaNs
features.fillna(features.mean(), inplace=True)  # Fill NaNs with the mean value of each column

test_loss, test_mae = model.evaluate(X_test, y_test)
print(f"Test MSE: {test_loss}")
print(f"Test MAE: {test_mae}")

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Model Loss Over Epochs')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.show()

predictions = model.predict(X_test)

print(history.history['loss'])
print(history.history['val_loss'])

test_mse, test_mae = model.evaluate(X_test, y_test)
print(f'Test MAE: {test_mae}')

import os
import cv2
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.metrics import MeanAbsoluteError

# Define the function to load and resize images
def load_and_resize_image(filename):
    image_path = os.path.join(dataset_path, 'training_set', filename)
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is not None:
        return cv2.resize(img, (800, 540))
    else:
        print(f"Failed to load image: {image_path}")
        return None

# Load annotations dataframe
annotations_df = pd.read_csv('training_set_pixel_size_and_HC.csv')  # Assuming you have the annotations CSV file

# Apply function to all filenames in the dataframe
images = [load_and_resize_image(filename) for filename in annotations_df['filename']]
images = [img for img in images if img is not None]

if not images:
    print("No images were loaded. Check file paths and names.")
else:
    # Convert list of images to a numpy array and preprocess
    images = np.array(images, dtype='float32').reshape(-1, 800, 540, 1) / 255.0
    hcs = annotations_df['head circumference (mm)'].values.astype('float32')

    # Splitting dataset
    train_images, val_images, train_hcs, val_hcs = train_test_split(images, hcs, test_size=0.2, random_state=42)

    print(f"Successfully loaded and split {len(images)} images.")
    print(f"Train set images: {len(train_images)}, Validation set images: {len(val_images)}")

    # Build the CNN model
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(800, 540, 1)),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(64, activation='relu'),
        Dense(1)  # Output layer for regression
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[MeanAbsoluteError()])

    # Train the model
    model.fit(train_images, train_hcs, validation_data=(val_images, val_hcs), epochs=10, batch_size=32)

    # Evaluate the model
    loss, mae = model.evaluate(val_images, val_hcs)
    print(f'Validation MAE: {mae:.2f}')

